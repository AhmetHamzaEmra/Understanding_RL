{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilize W and b\n",
    "n_input = 4\n",
    "n_hidden = 16\n",
    "n_output = env.action_space.n\n",
    "W1 = np.random.randn(n_input, n_hidden)\n",
    "b1 = np.ones([n_hidden])\n",
    "W2 = np.random.randn(n_hidden,n_output)\n",
    "b2 = np.zeros([n_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(obs):\n",
    "    # Forward pass \n",
    "    # two layered network with relu activation \n",
    "    N, D = obs.shape\n",
    "    h = obs.dot(W1) + b1 \n",
    "    h[h<0] = 0 \n",
    "    out_linear = h.dot(W2) + b2 \n",
    "    exp_scores = np.exp(out_linear)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) \n",
    "    # We dont always want to take the best posible action \n",
    "    # To explore better options we will choose the action\n",
    "    # randomly with predicted probablities\n",
    "    action = np.random.choice([x for x in range(n_output)], 1, p=probs[-1])\n",
    "    \n",
    "    # backward pass \n",
    "    grads={}\n",
    "    dscores = probs.copy()\n",
    "    dscores[range(N), list(action)] -= 1\n",
    "    dscores /= N\n",
    "    grads['W2'] = h.T.dot(dscores) \n",
    "    grads['b2'] = np.sum(dscores, axis = 0)\n",
    "    dh = dscores.dot( W2.T)\n",
    "    dh_ReLu = (h > 0) * dh\n",
    "    grads['W1'] = obs.T.dot(dh_ReLu) \n",
    "    grads['b1'] = np.sum(dh_ReLu, axis = 0)\n",
    "    return grads, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(obs):\n",
    "    # get action when rendering (testing)\n",
    "    # Forward pass\n",
    "    obs = obs.reshape([1,n_input])\n",
    "    h = obs.dot(W1) + b1 \n",
    "    h[h<0] = 0 \n",
    "    out_linear = h.dot(W2) + b2 \n",
    "    exp_scores = np.exp(out_linear)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return np.random.choice([x for x in range(n_output)], 1, p=probs[-1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(r, gamma = 0.7):\n",
    "    # We dont know which actions cause the rewards \n",
    "    # so we need to make couple previous actions also responsible \n",
    "    discounted = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add*gamma + r[t]\n",
    "        discounted[t] = running_add\n",
    "    return discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 99 / 100: Mean Score 138.150000"
     ]
    }
   ],
   "source": [
    "n_iter = 100 # we have early stop\n",
    "callback = 195 # consider the game is solved\n",
    "n_game_per_iter = 100\n",
    "n_action_per_game = 200\n",
    "learning_rate = 1e-1\n",
    "for itern in range(n_iter):\n",
    "    update_grads = {}\n",
    "    update_grads['W1'] = np.zeros_like(W1)\n",
    "    update_grads['b1'] = np.zeros_like(b1)\n",
    "    update_grads['W2'] = np.zeros_like(W2)\n",
    "    update_grads['b2'] = np.zeros_like(b2)\n",
    "    mean_reward = 0\n",
    "    all_gradients = []\n",
    "    for game in range(n_game_per_iter):\n",
    "        obs = env.reset()\n",
    "        current_rewards = []\n",
    "        current_gradients = []\n",
    "        total_reward = 0\n",
    "        for step in range(n_action_per_game):\n",
    "            obs = obs.reshape([1,n_input])\n",
    "            grads, action = training_step(obs)\n",
    "            obs, reward, done, info = env.step(action[0])\n",
    "            current_rewards.append(reward)\n",
    "            current_gradients.append(grads)\n",
    "            total_reward+=reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        mean_reward+=total_reward\n",
    "        current_rewards = np.array(current_rewards)\n",
    "        current_gradients = np.array(current_gradients)\n",
    "        # normalize the rewards \n",
    "        discounted_rewards = discount(current_rewards)\n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        discounted_rewards /= np.std(discounted_rewards)\n",
    "        # mulitply them with gradients \n",
    "        for i in range(current_gradients.shape[0]):\n",
    "            for _, n in enumerate(current_gradients[i]):\n",
    "                current_gradients[i][n] = current_gradients[i][n]*discounted_rewards[i]\n",
    "                all_gradients.append(current_gradients[i])\n",
    "    # take the mean gradient and make the update      \n",
    "    for i in range(len(all_gradients)):\n",
    "        for _, n in enumerate(all_gradients[i]):\n",
    "            update_grads[n]+=all_gradients[i][n]\n",
    "    for _, n in enumerate(update_grads):\n",
    "        update_grads[n]/= len(all_gradients)\n",
    "    W1 -= learning_rate*update_grads['W1']\n",
    "    b1 -= learning_rate*update_grads['b1']\n",
    "    W2 -= learning_rate*update_grads['W2']\n",
    "    b2 -= learning_rate*update_grads['b2']\n",
    "    \n",
    "    print('\\riteration %d / %d: Mean Score %f'% (itern, n_iter, mean_reward/n_game_per_iter), end = \"\")\n",
    "    # if the mean score of 100 games is higher than 195 \n",
    "    # consider solved\n",
    "    if mean_reward/n_game_per_iter > callback:\n",
    "        print(\"Training done!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1, Total Reward 195.000000\n",
      "Game 2, Total Reward 110.000000\n",
      "Game 3, Total Reward 143.000000\n",
      "Game 4, Total Reward 110.000000\n",
      "Game 5, Total Reward 112.000000\n",
      "Game 6, Total Reward 158.000000\n",
      "Game 7, Total Reward 134.000000\n",
      "Game 8, Total Reward 144.000000\n",
      "Game 9, Total Reward 171.000000\n",
      "Game 10, Total Reward 132.000000\n"
     ]
    }
   ],
   "source": [
    "# Render the env\n",
    "\n",
    "n_test = 10\n",
    "for i in range(n_test):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = get_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward+=reward\n",
    "        if done :\n",
    "            break\n",
    "    print(\"Game %d, Total Reward %f\"%(i+1, total_reward))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
