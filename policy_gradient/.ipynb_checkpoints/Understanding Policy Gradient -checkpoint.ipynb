{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilize W and b\n",
    "n_input = 4\n",
    "n_hidden = 8\n",
    "n_output = env.action_space.n\n",
    "W1 = np.random.randn(n_input, n_hidden)*0.1\n",
    "b1 = np.ones([n_hidden])\n",
    "W2 = np.random.randn(n_hidden,n_output)*0.1\n",
    "b2 = np.zeros([n_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(obs):\n",
    "    N, D = obs.shape\n",
    "    h = obs.dot(W1) + b1 \n",
    "    h[h<0] = 0 \n",
    "    out_linear = h.dot(W2) + b2 \n",
    "    exp_scores = np.exp(out_linear)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) \n",
    "    action = np.random.choice([x for x in range(n_output)], 1, p=probs[-1])\n",
    "\n",
    "    grads={}\n",
    "    dscores = probs.copy()\n",
    "    dscores[range(N), list(action)] -= 1\n",
    "    dscores /= N\n",
    "    grads['W2'] = h.T.dot(dscores) \n",
    "    grads['b2'] = np.sum(dscores, axis = 0)\n",
    "\n",
    "    dh = dscores.dot( W2.T)\n",
    "    dh_ReLu = (h > 0) * dh\n",
    "    grads['W1'] = obs.T.dot(dh_ReLu) \n",
    "    grads['b1'] = np.sum(dh_ReLu, axis = 0)\n",
    "    return grads, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(obs):\n",
    "    obs = obs.reshape([1,n_input])\n",
    "    h = obs.dot(W1) + b1 \n",
    "    h[h<0] = 0 \n",
    "    out_linear = h.dot(W2) + b2 \n",
    "    exp_scores = np.exp(out_linear)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) \n",
    "    return np.random.choice([x for x in range(n_output)], 1, p=probs[-1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(r, gamma = 0.7):\n",
    "    discounted = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add*gamma + r[t]\n",
    "        discounted[t] = running_add\n",
    "    return discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 267 / 10000: Mean Score 23.680000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4c3058a2c5f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_action_per_game\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mcurrent_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-cc7cdc3c45b7>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(obs)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mout_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mexp_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_linear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_iter = 10000\n",
    "n_game_per_iter = 100\n",
    "n_action_per_game = 200\n",
    "learning_rate = 1e-3\n",
    "for itern in range(n_iter):\n",
    "    update_grads = {}\n",
    "    update_grads['W1'] = np.zeros_like(W1)\n",
    "    update_grads['b1'] = np.zeros_like(b1)\n",
    "    update_grads['W2'] = np.zeros_like(W2)\n",
    "    update_grads['b2'] = np.zeros_like(b2)\n",
    "    mean_reward = 0\n",
    "    all_gradients = []\n",
    "    for game in range(n_game_per_iter):\n",
    "        obs = env.reset()\n",
    "        current_rewards = []\n",
    "        current_gradients = []\n",
    "        total_reward = 0\n",
    "        for step in range(n_action_per_game):\n",
    "            obs = obs.reshape([1,n_input])\n",
    "            grads, action = training_step(obs)\n",
    "            obs, reward, done, info = env.step(action[0])\n",
    "            current_rewards.append(reward)\n",
    "            current_gradients.append(grads)\n",
    "            total_reward+=reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        mean_reward+=total_reward\n",
    "        current_rewards = np.array(current_rewards)\n",
    "        current_gradients = np.array(current_gradients)\n",
    "        discounted_rewards = discount(current_rewards)\n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        discounted_rewards /= np.std(discounted_rewards)\n",
    "        \n",
    "        for i in range(current_gradients.shape[0]):\n",
    "            for _, n in enumerate(current_gradients[i]):\n",
    "                current_gradients[i][n] = current_gradients[i][n]*discounted_rewards[i]\n",
    "                all_gradients.append(current_gradients[i])\n",
    "                \n",
    "    for i in range(len(all_gradients)):\n",
    "        for _, n in enumerate(all_gradients[i]):\n",
    "            update_grads[n]+=all_gradients[i][n]\n",
    "    for _, n in enumerate(update_grads):\n",
    "        update_grads[n]/= len(all_gradients)\n",
    "    W1 -= learning_rate*update_grads['W1']\n",
    "    b1 -= learning_rate*update_grads['b1']\n",
    "    W2 -= learning_rate*update_grads['W2']\n",
    "    b2 -= learning_rate*update_grads['b2']\n",
    "    \n",
    "    print('\\riteration %d / %d: Mean Score %f'% (itern, n_iter, mean_reward/n_game_per_iter), end = \"\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the env\n",
    "\n",
    "n_test = 10\n",
    "for i in range(n_test):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = get_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward+=reward\n",
    "        if done :\n",
    "            break\n",
    "    print(\"Game %d, Total Reward %f\"%(i+1, total_reward))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
