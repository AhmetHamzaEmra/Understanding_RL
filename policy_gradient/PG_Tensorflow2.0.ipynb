{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic NN with one hidden layer \n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(32, input_dim = 4, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(2, activation = \"softmax\"))\n",
    "model.build()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "compute_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma = 0.8):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode  0  Score  14.0\n",
      "Episode  100  Score  20.22\n",
      "Episode  200  Score  35.29\n",
      "Episode  300  Score  36.55\n",
      "Episode  400  Score  54.4\n",
      "Episode  500  Score  68.5\n",
      "Episode  600  Score  150.5\n",
      "Episode  700  Score  150.5\n",
      "Episode  800  Score  45.94\n",
      "Episode  900  Score  39.7\n",
      "Episode  1000  Score  42.82\n",
      "Episode  1100  Score  133.5\n",
      "Episode  1200  Score  122.5\n",
      "Episode  1300  Score  130.5\n",
      "Episode  1400  Score  150.5\n",
      "Episode  1500  Score  150.5\n",
      "Episode  1600  Score  150.5\n",
      "Episode  1700  Score  150.5\n",
      "Episode  1800  Score  81.5\n",
      "Episode  1900  Score  67.5\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "episodes = 2000\n",
    "scores = []\n",
    "update_every = 5\n",
    "\n",
    "gradBuffer = model.trainable_variables\n",
    "for ix,grad in enumerate(gradBuffer):\n",
    "    gradBuffer[ix] = grad * 0  \n",
    "    \n",
    "for e in range(episodes):\n",
    "    s = env.reset()\n",
    "    ep_memory = []\n",
    "    ep_score = 0\n",
    "    done = False \n",
    "    while not done: \n",
    "        s = s.reshape([1,4])\n",
    "        with tf.GradientTape() as tape:\n",
    "            #forward pass\n",
    "            logits = model(s)\n",
    "            a_dist = logits.numpy()\n",
    "            # Choose random action with p = action dist\n",
    "            a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "            a = np.argmax(a_dist == a)\n",
    "            loss = compute_loss([a], logits)\n",
    "        # make the choosen action \n",
    "        s, r, done, _ = env.step(a)\n",
    "        ep_score +=r\n",
    "        if done: r-=10\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        ep_memory.append([grads,r])\n",
    "        scores.append(ep_score)\n",
    "    # Discound the rewards \n",
    "    ep_memory = np.array(ep_memory)\n",
    "    ep_memory[:,1] = discount_rewards(ep_memory[:,1])\n",
    "\n",
    "    for grads, r in ep_memory:\n",
    "        for ix,grad in enumerate(grads):\n",
    "            gradBuffer[ix] += grad * r\n",
    "\n",
    "    if e % update_every == 0:\n",
    "        optimizer.apply_gradients(zip(gradBuffer, model.trainable_variables))\n",
    "        for ix,grad in enumerate(gradBuffer):\n",
    "            gradBuffer[ix] = grad * 0\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        print(\"Episode  {}  Score  {}\".format(e, np.mean(scores[-100:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
